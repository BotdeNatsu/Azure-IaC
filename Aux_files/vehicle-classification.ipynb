{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from copy import deepcopy\n",
    "# import json\n",
    "\n",
    "# import requests\n",
    "# import numpy as np\n",
    "\n",
    "# # Azure Services\n",
    "# from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "# from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "# from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "# from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# # Visualization\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['figure.figsize'] = (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPLATE = {\n",
    "#     'status': None,\n",
    "#     'img_location': None,\n",
    "#     \"applied_fuctions\": [],\n",
    "#     'n_info': 0,\n",
    "#     'vehicle_type': {\n",
    "#         'label': None,\n",
    "#         'probability': None\n",
    "#     },\n",
    "#     'photo_type': {\n",
    "#         'label': None,\n",
    "#         'probability': None\n",
    "#     },\n",
    "#     'vin': {\n",
    "#         'text': None,\n",
    "#         'probability': None\n",
    "#     },\n",
    "#     'plate': {\n",
    "#         'text': None,\n",
    "#         'probability': None\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VEHICLE_TYPES = ['Automóvil', 'Camión', 'Motocicleta']\n",
    "# PHOTO_TYPES = [\n",
    "#     'FRONTAL R2'  , 'FRONTAL R4'  ,\n",
    "#     'TRASERA R2'  , 'TRASERA R4'  ,\n",
    "#     'DERECHA R2'  , 'DERECHA R4'  ,\n",
    "#     'IZQUIERDA R2', 'IZQUIERDA R4',\n",
    "#     'INTERIOR R2' , 'INTERIOR R4' ,\n",
    "#     'VIN',\n",
    "#     'DOCUMENTO'\n",
    "# ]\n",
    "# # camara_angles = [\n",
    "# #     'FRONTAL',\n",
    "# #     'TRASERA',\n",
    "# #     'DERECHA',\n",
    "# #     'IZQUIERDA',\n",
    "# #     'INTERIOR',\n",
    "# # ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# HEADERS_CUSTOM_VISION = {\n",
    "#     'Content-Type': 'application/octet-stream',\n",
    "#     'Prediction-Key': '5ceef9f635804c05ae36fdca6dd99eaf'\n",
    "# }\n",
    "\n",
    "# # Classification -------------------------------------------------------\n",
    "# PROJECT_NAME = 'Iteration3%20%7C%20F2M2'\n",
    "# PROJECT_ID = '077a7a83-5a6f-4a47-a946-374814e786f4'\n",
    "# ENDPOINT_CLASSIFICATION = (\n",
    "#     f'https://cstvtclasificadorimagentesdeveastus2-prediction.'\n",
    "#     f'cognitiveservices.azure.com/customvision/v3.0/Prediction/'\n",
    "#     f'{PROJECT_ID}/classify/iterations/{PROJECT_NAME}/image'\n",
    "# )\n",
    "# THRESHOLD_CLASSIFICATION = 0.7\n",
    "# # ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# # Plate Recognition ----------------------------------------------------\n",
    "# PROJECT_NAME = 'Iteration2%20%7C%20F2M1'\n",
    "# PROJECT_ID = 'ca4fe2cf-23ff-44ab-8604-9a3b092caecf'\n",
    "# ENDPOINT_PLATE_DETECTION = (\n",
    "#     f'https://cstvtclasificadorimagentesdeveastus2-prediction.'\n",
    "#     f'cognitiveservices.azure.com/customvision/v3.0/Prediction/'\n",
    "#     f'{PROJECT_ID}/detect/iterations/{PROJECT_NAME}/image'\n",
    "# )\n",
    "# THRESHOLD_PLATE_DETECTION = 0.94\n",
    "# # ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plate Number Extraction ----------------------------------------------\n",
    "# ENDPOINT_PLATE_EXTRACTION = (\n",
    "#     'https://cv-clasificadorimagenes-dev-eastus2-01.'\n",
    "#     'cognitiveservices.azure.com/'\n",
    "# )\n",
    "# KEY_PLATE_EXTRACTION = 'a25a975c79e1418cbde0a23b4021b74e'\n",
    "# # ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENDPOINT_DOCUMENT_INTELLIGENCE = 'https://eastus2.api.cognitive.microsoft.com/'\n",
    "# KEY_DOCUMENT_INTELLIGENCE = '1bcc168accb740f09a59b7313db475a4'\n",
    "# MODEL_ID = 'Iteracion2-neural'\n",
    "\n",
    "# THRESHOLD_DOC_EXTRACTION = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classify_image(\n",
    "#     results: dict,\n",
    "#     img_is_url: bool = False\n",
    "#     ) -> dict:\n",
    "#     \"\"\"\n",
    "#     Classifies an image using the Azure AI Custom Vision API and updates the \n",
    "#     results dictionary with the most probable predictions.\n",
    "\n",
    "#     This function takes an image location, which can be either a local file \n",
    "#     path or a URL, and sends it to the Azure AI Custom Vision API for \n",
    "#     classification. The API returns multiple predictions, each associated \n",
    "#     with a probability score. The predictions are filtered based on a \n",
    "#     probability threshold (`THRESHOLD_CLASSIFICATION`) and sorted to ensure \n",
    "#     that the most probable predictions are included in the results. The \n",
    "#     function updates the results dictionary with the classified labels and \n",
    "#     their corresponding probabilities.\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     img_location : str\n",
    "#         The file path or URL of the image to be classified.\n",
    "#     results : dict\n",
    "#         A dictionary template to store the classification results, by default\n",
    "#         template.\n",
    "#     img_is_url : bool, optional\n",
    "#         Indicates whether `img_location` is a URL or a local file path, by \n",
    "#         default False.\n",
    "\n",
    "#     Returns:\n",
    "#     --------\n",
    "#     dict\n",
    "#         The updated results dictionary containing the classification labels \n",
    "#         and probabilities.\n",
    "    \n",
    "#     \"\"\"\n",
    "#     results['applied_fuctions'].append('classify_image')\n",
    "#     img_location = results['img_location']\n",
    "\n",
    "#     # Call Azure AI Custom vision API (multi-label) --------------------\n",
    "#     if img_is_url:\n",
    "#         pass\n",
    "\n",
    "#     else:\n",
    "#         with open(img_location, 'rb') as img_binary:\n",
    "#             response = requests.request( \n",
    "#                 method=\"POST\", \n",
    "#                 url=ENDPOINT_CLASSIFICATION,\n",
    "#                 headers=HEADERS_CUSTOM_VISION, \n",
    "#                 data=img_binary\n",
    "#             )\n",
    "#     # ------------------------------------------------------------------\n",
    "\n",
    "#     # The predictions are ordered from lowest to highest probability to \n",
    "#     # guarantee that the most probable values will be present in \n",
    "#     # 'results', regardless of the number of labels predicted. \n",
    "#     predictions = response.json()['predictions'][::-1]\n",
    "\n",
    "#     for prediction in predictions:  \n",
    "            \n",
    "#         if prediction['probability'] >= THRESHOLD_CLASSIFICATION:\n",
    "\n",
    "#             if prediction['tagName'] in VEHICLE_TYPES:\n",
    "#                 results['vehicle_type']['label']       = prediction['tagName']\n",
    "#                 results['vehicle_type']['probability'] = prediction['probability']\n",
    "\n",
    "#             else:\n",
    "#                 results['photo_type']['label']         = prediction['tagName'].split()[0]\n",
    "#                 results['photo_type']['probability']   = prediction['probability']\n",
    "\n",
    "#             results['n_info'] += 1\n",
    "\n",
    "#     return results        \n",
    "\n",
    "\n",
    "# # results1 = classify_image(IMG_PATH)\n",
    "# # pprint(results1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plate Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def crop_image(\n",
    "#     img_location,\n",
    "#     image,\n",
    "#     bounding_box\n",
    "#     ):\n",
    "\n",
    "#     # Extract region defined by the bounding box\n",
    "#     left   = bounding_box['left']   * image.width\n",
    "#     top    = bounding_box['top']    * image.height\n",
    "#     width  = bounding_box['width']  * image.width\n",
    "#     height = bounding_box['height'] * image.height\n",
    "\n",
    "#     # Crop the image\n",
    "#     cropped_image = image.crop((\n",
    "#         left, \n",
    "#         top, \n",
    "#         left + width, \n",
    "#         top + height\n",
    "#     ))\n",
    "\n",
    "#     # Plot the cropped image\n",
    "#     plt.imshow(cropped_image)\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig(\n",
    "#         f'work-zone/Resultados/{os.path.basename(img_location)}', \n",
    "#         bbox_inches='tight', \n",
    "#         pad_inches = 0\n",
    "#     )\n",
    "#     plt.show()\n",
    "\n",
    "#     return cropped_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def detect_plate(\n",
    "#     results: dict,\n",
    "#     img_is_url: bool = False  \n",
    "#     ) -> dict:\n",
    "#     \"\"\"\n",
    "#     Detects and extracts license plates from an image using the Azure AI Custom \n",
    "#     Vision API for object detection.\n",
    "#     \"\"\"\n",
    "#     img_location = results['img_location']\n",
    "#     results['applied_fuctions'].append('detect_plate')\n",
    "\n",
    "#     # Call Azure AI Custom vision API (object-detection) ---------------\n",
    "#     if img_is_url:\n",
    "#         pass\n",
    "#     else:\n",
    "#         with open(img_location, 'rb') as img_binary:\n",
    "#             original_image = Image.open(img_location)\n",
    "#             response = requests.request(\n",
    "#                 method=\"POST\", \n",
    "#                 url=ENDPOINT_PLATE_DETECTION, \n",
    "#                 headers=HEADERS_CUSTOM_VISION, \n",
    "#                 data=img_binary\n",
    "#             )\n",
    "#     # ------------------------------------------------------------------\n",
    "\n",
    "#     predictions = response.json()['predictions']\n",
    "\n",
    "#     if predictions and (predictions[0]['probability'] \n",
    "#                         >= THRESHOLD_PLATE_DETECTION\n",
    "#                        ):\n",
    "#         bounding_box = predictions[0]['boundingBox']\n",
    "#         cropped_image = crop_image(\n",
    "#             img_location, \n",
    "#             original_image, \n",
    "#             bounding_box\n",
    "#         )\n",
    "\n",
    "#     else:  # NO PLATE DETECTED\n",
    "#         results['status'] = 'REVIEW'\n",
    "#         cropped_image = None\n",
    "\n",
    "#     return results, cropped_image \n",
    "        \n",
    "\n",
    "# # results2, cropped_image = detect_plate(IMG_PATH, results=results1)\n",
    "# # results2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plate Number Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an Image Analysis client\n",
    "# client = ImageAnalysisClient(\n",
    "#     endpoint=ENDPOINT_PLATE_EXTRACTION,\n",
    "#     credential=AzureKeyCredential(KEY_PLATE_EXTRACTION)\n",
    "# )\n",
    "\n",
    "# # Load image to analyze into a 'bytes' object\n",
    "# with open(f'work-zone/Resultados/{os.path.basename(IMG_PATH)}', \"rb\") as f:\n",
    "#     image_data = f.read()\n",
    "\n",
    "# # Extract text (OCR) from an image stream. This will be a synchronously (blocking) call.\n",
    "# result = client.analyze(\n",
    "#     image_data=image_data,\n",
    "#     visual_features=[VisualFeatures.READ]\n",
    "# )\n",
    "\n",
    "# result_lines = result.read['blocks'][0]['lines']\n",
    "# pprint(result_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIN Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_text(\n",
    "#     content_type: str, \n",
    "#     results: dict, \n",
    "#     img_is_url: bool\n",
    "#     ):\n",
    "#     \"\"\"\n",
    "#     docstring\n",
    "#     \"\"\"\n",
    "#     img_location = results['img_location']\n",
    "#     results['applied_fuctions'].append('extract_text')\n",
    "\n",
    "#     # Call Azure AI Vision API (OCR) -----------------------------------\n",
    "#     client = ImageAnalysisClient(\n",
    "#         endpoint=ENDPOINT_PLATE_EXTRACTION,\n",
    "#         credential=AzureKeyCredential(KEY_PLATE_EXTRACTION)\n",
    "#     )\n",
    "\n",
    "#     if img_is_url:\n",
    "#         pass\n",
    "#     else:\n",
    "#         with open(img_location, \"rb\") as f:\n",
    "#             image_data = f.read()\n",
    "\n",
    "#         # Extract text (OCR) from an image stream. This will be a synchronously (blocking) call.\n",
    "#         response = client.analyze(\n",
    "#             image_data=image_data,\n",
    "#             visual_features=[VisualFeatures.READ]\n",
    "#         )\n",
    "#     # ------------------------------------------------------------------\n",
    "    \n",
    "#     lines = response.read['blocks'][0]['lines']\n",
    "\n",
    "#     results[content_type]['text'] = str(lines)\n",
    "\n",
    "#     return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an Image Analysis client\n",
    "# client = ImageAnalysisClient(\n",
    "#     endpoint=ENDPOINT_PLATE_EXTRACTION,\n",
    "#     credential=AzureKeyCredential(KEY_PLATE_EXTRACTION)\n",
    "# )\n",
    "\n",
    "# with open(IMG_PATH, \"rb\") as f:\n",
    "#     image_data = f.read()\n",
    "\n",
    "# # Extract text (OCR) from an image stream. This will be a synchronously (blocking) call.\n",
    "# result = client.analyze(\n",
    "#     image_data=image_data,\n",
    "#     visual_features=[VisualFeatures.READ]\n",
    "# )\n",
    "# result_lines = result.read['blocks'][0]['lines']\n",
    "# pprint(result_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_from_doc(\n",
    "#         results,\n",
    "#         img_is_url\n",
    "#         ):\n",
    "#     \"\"\"\n",
    "#     Remember to remove the key from your code when you're done, and \n",
    "#     never post it publicly. For production, use secure methods to store\n",
    "#     and access your credentials. For more information, see\n",
    "#     https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-security?tabs=command-line%2Ccsharp#environment-variables-and-application-configuration\n",
    "#     \"\"\"\n",
    "#     img_location = results['img_location']\n",
    "#     results['applied_fuctions'].append('extract_from_doc')\n",
    "\n",
    "#     # Call Azure AI Document Intelligence API --------------------------\n",
    "#     client = DocumentAnalysisClient(\n",
    "#         endpoint=ENDPOINT_DOCUMENT_INTELLIGENCE, \n",
    "#         credential=AzureKeyCredential(KEY_DOCUMENT_INTELLIGENCE)\n",
    "#     )\n",
    "#     if img_is_url:\n",
    "#         response = client.begin_analyze_document_from_url(\n",
    "#             model_id=MODEL_ID, \n",
    "#             document_url=img_location\n",
    "#         )\n",
    "    \n",
    "#     else:\n",
    "#         with open(img_location, 'rb') as f:\n",
    "#             image_data = f.read()\n",
    "            \n",
    "#             response = client.begin_analyze_document(\n",
    "#                 model_id=MODEL_ID, \n",
    "#                 document=image_data\n",
    "#             )\n",
    "#     # ------------------------------------------------------------------\n",
    "    \n",
    "#     fields = response.result().documents[0].fields\n",
    "#     plate_confidence = fields['Placa'].confidence\n",
    "#     vin_confidence   = fields['VIN'].confidence\n",
    "#     # error_message = 'ERROR WITH'\n",
    "\n",
    "#     # if plate_confidence >= THRESHOLD_DOC_EXTRACTION:\n",
    "#     results['plate']['text'] = fields['Placa'].value\n",
    "#     results['plate']['probability'] = plate_confidence\n",
    "#     # else:\n",
    "#         # error_message += ' PLATE'\n",
    "\n",
    "#     # if vin_confidence >= THRESHOLD_DOC_EXTRACTION:\n",
    "#     results['vin']['text'] = fields['VIN'].value\n",
    "#     results['vin']['probability'] = vin_confidence\n",
    "#     # else:\n",
    "#         # error_message += ' VIN'\n",
    "\n",
    "#     # if not error_message:\n",
    "#     results['status'] = 'TEST'\n",
    "#     # else:\n",
    "#     #     results['status'] = 'REPEAT'\n",
    "#     #     results['message'] = error_message\n",
    "\n",
    "#     return results\n",
    "\n",
    "\n",
    "# # extract_from_doc(IMG_PATH, img_is_url=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base64\n",
    "# from PIL import Image\n",
    "# from io import BytesIO\n",
    "\n",
    "# def base64_to_image(base64_string):\n",
    "#     # Decode the base64 string to bytes\n",
    "#     img_data = base64.b64decode(base64_string)\n",
    "#     # Create a BytesIO object from the bytes\n",
    "#     img_buffer = BytesIO(img_data)\n",
    "#     # Open the image using Pillow\n",
    "#     image = Image.open(img_buffer)\n",
    "#     return image\n",
    "\n",
    "# def display_image(image):\n",
    "#     image.show()\n",
    "\n",
    "# # Example base64 string (replace with your actual base64 string)\n",
    "# # base64_string = 'iVBORw0KGgoAAAANSUhEUgAAAAUA...'\n",
    "\n",
    "# # Convert base64 string to image\n",
    "# image = base64_to_image(base64_string)\n",
    "\n",
    "# # Display the image\n",
    "# display_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "with open(IMG_PATH, 'rb') as img_binary:\n",
    "    input_request = {\n",
    "        'files': [img_binary]\n",
    "    }\n",
    "    print(io.FileIO(img_binary.fileno()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS_CUSTOM_VISION = {\n",
    "    'Content-Type': 'application/octet-stream',\n",
    "    'Prediction-Key': '5ceef9f635804c05ae36fdca6dd99eaf'\n",
    "}\n",
    "\n",
    "PROJECT_NAME = 'Iteration3%20%7C%20F2M2'\n",
    "PROJECT_ID = '077a7a83-5a6f-4a47-a946-374814e786f4'\n",
    "ENDPOINT_CLASSIFICATION = (\n",
    "    f'https://cstvtclasificadorimagentesdeveastus2-prediction.'\n",
    "    f'cognitiveservices.azure.com/customvision/v3.0/Prediction/'\n",
    "    f'{PROJECT_ID}/classify/iterations/{PROJECT_NAME}/image'\n",
    ")\n",
    "\n",
    "import requests\n",
    "IMG_PATH = 'work-zone/Fase2/Automóvil/FRONTAL/1BLSR5_RFFRE.jpg'\n",
    "\n",
    "class Example:\n",
    "    def __init__(self, img_binary):\n",
    "        self.img_binary = img_binary\n",
    "        self.content = {'applied_fuctions': []}\n",
    "    \n",
    "    def foo(self):\n",
    "        try:\n",
    "            response = requests.request( \n",
    "                method='POST', \n",
    "                url=ENDPOINT_CLASSIFICATION,\n",
    "                headers=HEADERS_CUSTOM_VISION, \n",
    "                data=self.img_binary\n",
    "            )\n",
    "            self.content['applied_fuctions'].append('classify_vehicle_photo')\n",
    "            response.raise_for_status()\n",
    "\n",
    "            return response\n",
    "        \n",
    "        except Exception as x:\n",
    "            raise x\n",
    "    \n",
    "    def classify_vehicle_photo(self):\n",
    "        \"\"\"\n",
    "        Classifies an image using the Azure AI Custom Vision API and updates \n",
    "        the self.content dictionary with the most probable predictions.\n",
    "        \"\"\"\n",
    "        self.content['applied_fuctions'].append('classify_vehicle_photo')\n",
    "\n",
    "        # Call Azure AI Custom vision API (multi-label) ----------------\n",
    "        response = requests.request( \n",
    "            method='POST', \n",
    "            url=ENDPOINT_CLASSIFICATION,\n",
    "            headers=HEADERS_CUSTOM_VISION, \n",
    "            data=self.img_binary#img_binary\n",
    "        )\n",
    "\n",
    "        print(response.json())\n",
    "\n",
    "# pprint(response.json())\n",
    "with open(IMG_PATH, 'rb') as img_binary:\n",
    "    input_request = {\n",
    "        'files': [img_binary]\n",
    "    }\n",
    "    # ex = Example(input_request['files'][0])\n",
    "    img = Example(input_request['files'][0])\n",
    "    img.classify_vehicle_photo()\n",
    "    # response = ex.foo()\n",
    "img.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleImage:\n",
    "\n",
    "    def __init__(self, img_binary):\n",
    "        self.img_binary = img_binary\n",
    "        self.content = {\n",
    "            'status'          : None,\n",
    "            'img_location'    : img_binary,\n",
    "            'applied_fuctions': [],\n",
    "            'n_info'          : 0,\n",
    "            'vehicle_type'    : {'label': None, 'probability': None},\n",
    "            'photo_type'      : {'label': None, 'probability': None},\n",
    "            'vin'             : {'text' : None, 'probability': None},\n",
    "            'plate'           : {'text' : None, 'probability': None}\n",
    "        }\n",
    "\n",
    "\n",
    "    def classify_vehicle_photo(self):\n",
    "        \"\"\"\n",
    "        Classifies an image using the Azure AI Custom Vision API and updates \n",
    "        the self.content dictionary with the most probable predictions.\n",
    "        \"\"\"\n",
    "        self.content['applied_fuctions'].append('classify_vehicle_photo')\n",
    "\n",
    "        # Call Azure AI Custom vision API (multi-label) ----------------\n",
    "        response = requests.request( \n",
    "            method='POST', \n",
    "            url=ENDPOINT_CLASSIFICATION,\n",
    "            headers=HEADERS_CUSTOM_VISION, \n",
    "            data=self.img_binary#img_binary\n",
    "        )\n",
    "\n",
    "        # The predictions are ordered from lowest to highest probability \n",
    "        # to guarantee that the most probable values will be present in \n",
    "        # 'self.content', regardless of the number of labels predicted. \n",
    "        predictions = response.json()['predictions'][::-1]\n",
    "\n",
    "        for prediction in predictions:  \n",
    "                \n",
    "            if prediction['probability'] >= THRESHOLD_CLASSIFICATION:\n",
    "\n",
    "                if prediction['tagName'] in VEHICLE_TYPES:\n",
    "                    self.content['vehicle_type']['label'] = \\\n",
    "                        prediction['tagName']\n",
    "                    self.content['vehicle_type']['probability'] = \\\n",
    "                        prediction['probability']\n",
    "\n",
    "                else: # PHOTO_TYPES\n",
    "                    self.content['photo_type']['label'] = \\\n",
    "                        prediction['tagName'].split()[0]                # i.e. 'FRONTAL R4' -> 'FRONTAL'\n",
    "                    self.content['photo_type']['probability'] = \\\n",
    "                        prediction['probability']\n",
    "\n",
    "                self.content['n_info'] += 1\n",
    "    \n",
    "with open(IMG_PATH, 'rb') as img_binary:\n",
    "    input_request = {\n",
    "        'files': [img_binary]\n",
    "    }\n",
    "\n",
    "    img_example = VehicleImage(input_request['files'][0])\n",
    "    img_example.classify_vehicle_photo()\n",
    "\n",
    "pprint(img_example.content, width=200, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_example.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import requests\n",
    "\n",
    "# Azure Services\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Visualization\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#%% PARAMS\n",
    "VEHICLE_TYPES = ['Automóvil', 'Camión', 'Motocicleta']\n",
    "PHOTO_TYPES = [\n",
    "    'FRONTAL R2'  , 'FRONTAL R4'  ,\n",
    "    'TRASERA R2'  , 'TRASERA R4'  ,\n",
    "    'DERECHA R2'  , 'DERECHA R4'  ,\n",
    "    'IZQUIERDA R2', 'IZQUIERDA R4',\n",
    "    'INTERIOR R2' , 'INTERIOR R4' ,\n",
    "    'VIN',\n",
    "    'DOCUMENTO'\n",
    "]\n",
    "\n",
    "\n",
    "#%% Azure APIs\n",
    "# Custom Vision --------------------------------------------------------\n",
    "HEADERS_CUSTOM_VISION = {\n",
    "    'Content-Type': 'application/octet-stream',\n",
    "    'Prediction-Key': '5ceef9f635804c05ae36fdca6dd99eaf'\n",
    "}\n",
    "# Classification\n",
    "PROJECT_NAME = 'Iteration3%20%7C%20F2M2'\n",
    "PROJECT_ID = '077a7a83-5a6f-4a47-a946-374814e786f4'\n",
    "ENDPOINT_CLASSIFICATION = (\n",
    "    f'https://cstvtclasificadorimagentesdeveastus2-prediction.'\n",
    "    f'cognitiveservices.azure.com/customvision/v3.0/Prediction/'\n",
    "    f'{PROJECT_ID}/classify/iterations/{PROJECT_NAME}/image'\n",
    ")\n",
    "THRESHOLD_CLASSIFICATION = 0.7\n",
    "# Object Detection\n",
    "PROJECT_NAME = 'Iteration2%20%7C%20F2M1'\n",
    "PROJECT_ID = 'ca4fe2cf-23ff-44ab-8604-9a3b092caecf'\n",
    "ENDPOINT_PLATE_DETECTION = (\n",
    "    f'https://cstvtclasificadorimagentesdeveastus2-prediction.'\n",
    "    f'cognitiveservices.azure.com/customvision/v3.0/Prediction/'\n",
    "    f'{PROJECT_ID}/detect/iterations/{PROJECT_NAME}/image'\n",
    ")\n",
    "THRESHOLD_PLATE_DETECTION = 0.94\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Computer Vision ------------------------------------------------------\n",
    "ENDPOINT_PLATE_EXTRACTION = (\n",
    "    'https://cv-clasificadorimagenes-dev-eastus2-01.'\n",
    "    'cognitiveservices.azure.com/'\n",
    ")\n",
    "KEY_PLATE_EXTRACTION = 'a25a975c79e1418cbde0a23b4021b74e'\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Document Intelligence ------------------------------------------------\n",
    "ENDPOINT_DOCUMENT_INTELLIGENCE = 'https://eastus2.api.cognitive.microsoft.com/'\n",
    "KEY_DOCUMENT_INTELLIGENCE = '1bcc168accb740f09a59b7313db475a4'\n",
    "MODEL_ID = 'Iteracion2-neural'\n",
    "\n",
    "THRESHOLD_DOC_EXTRACTION = 0.5\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#%% Vision Toolkit - Vehicle Image Class\n",
    "class VehicleImage:\n",
    "\n",
    "    def __init__(self, img_binary):\n",
    "        self.img_binary = img_binary\n",
    "        self.content = {\n",
    "            'status'          : None,\n",
    "            'img_location'    : img_binary,\n",
    "            'applied_fuctions': [],\n",
    "            'n_info'          : 0,\n",
    "            'vehicle_type'    : {'label': None, 'probability': None},\n",
    "            'photo_type'      : {'label': None, 'probability': None},\n",
    "            'vin'             : {'text' : None, 'probability': None},\n",
    "            'plate'           : {'text' : None, 'probability': None}\n",
    "        }\n",
    "\n",
    "\n",
    "    def classify_vehicle_photo(self):\n",
    "        \"\"\"\n",
    "        Classifies an image using the Azure AI Custom Vision API and updates \n",
    "        the self.content dictionary with the most probable predictions.\n",
    "        \"\"\"\n",
    "        self.content['applied_fuctions'].append('classify_vehicle_photo')\n",
    "\n",
    "        # Call Azure AI Custom vision API (multi-label) ----------------\n",
    "        response = requests.request( \n",
    "            method='POST', \n",
    "            url=ENDPOINT_CLASSIFICATION,\n",
    "            headers=HEADERS_CUSTOM_VISION, \n",
    "            data=self.img_binary#img_binary\n",
    "        )\n",
    "        print(response.json())\n",
    "        # --------------------------------------------------------------\n",
    "\n",
    "        # The predictions are ordered from lowest to highest probability \n",
    "        # to guarantee that the most probable values will be present in \n",
    "        # 'self.content', regardless of the number of labels predicted. \n",
    "        # predictions = response.json()['predictions'][::-1]\n",
    "\n",
    "        # for prediction in predictions:  \n",
    "                \n",
    "        #     if prediction['probability'] >= THRESHOLD_CLASSIFICATION:\n",
    "\n",
    "        #         if prediction['tagName'] in VEHICLE_TYPES:\n",
    "        #             self.content['vehicle_type']['label'] = \\\n",
    "        #                 prediction['tagName']\n",
    "        #             self.content['vehicle_type']['probability'] = \\\n",
    "        #                 prediction['probability']\n",
    "\n",
    "        #         else: # PHOTO_TYPES\n",
    "        #             self.content['photo_type']['label'] = \\\n",
    "        #                 prediction['tagName'].split()[0]                # i.e. 'FRONTAL R4' -> 'FRONTAL'\n",
    "        #             self.content['photo_type']['probability'] = \\\n",
    "        #                 prediction['probability']\n",
    "\n",
    "        #         self.content['n_info'] += 1\n",
    "\n",
    "\n",
    "\n",
    "# from vision_toolkit import VehicleImage\n",
    "# from pprint import pprint\n",
    "# IMG_PATH = 'work-zone/Fase2/NEGATIVO/Tests/TEST FIAT - FRONTAL CALIDAD MEDIA-ALTA.jpeg'\n",
    "# with open(IMG_PATH, 'rb') as img_binary:\n",
    "#     input_request = {\n",
    "#         'files': [img_binary]\n",
    "#     }\n",
    "\n",
    "\n",
    "#     # Load image\n",
    "#     print(input_request['files'][0])\n",
    "#     img = VehicleImage(img_binary)\n",
    "#     img.show()\n",
    "\n",
    "#     img.classify_vehicle_photo()\n",
    "\n",
    "#     pprint(img.content, width=200, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG_PATH = 'work-zone/Fase2/Automóvil/FRONTAL/1BLSR5_RFFRE.jpg'\n",
    "# IMG_PATH = 'work-zone/Fase2/NEGATIVO/TEST FIAT - FRONTAL CALIDAD MEDIA-ALTA.jpeg'\n",
    "# IMG_PATH = 'work-zone/Fase2/NEGATIVO/TEST FIAT - VIN CALIDAD MEDIA-ALTA.jpeg' # LIM\n",
    "# IMG_PATH = 'work-zone/Fase2/Automóvil/VIN/YTRMMD_RFVIN.jpg'\n",
    "# IMG_PATH = 'work-zone/Fase2/Camión/TRASERA/WOS755 - TRASERA.png'\n",
    "# IMG_PATH = 'work-zone/Fase2/Camión/INTERIOR/WZD418 - INTERIOR.jpeg'\n",
    "\n",
    "# VIN\n",
    "# IMG_PATH = 'work-zone/Fase2/NEGATIVO/Tests/TEST FIAT - VIN CALIDAD ALTA.jpeg'\n",
    "\n",
    "#DOCUMENT\n",
    "# IMG_PATH = 'work-zone/Fase2/DOCUMENTO/VXQ671_RFRTV.jpg'                 # Plate/VIN\n",
    "# IMG_PATH = 'work-zone/Fase2/DOCUMENTO/xx2 - DOCUMENTO DEKRA.png'        # Only VIN\n",
    "# IMG_IS_URL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    # Open the image file\n",
    "    with Image.open(image_path) as image:\n",
    "        # Create a BytesIO object\n",
    "        buffered = BytesIO()\n",
    "        # Save the image to the BytesIO object in PNG format\n",
    "        image.save(buffered, format='PNG')\n",
    "        # Get the byte data from the BytesIO object\n",
    "        img_byte = buffered.getvalue()\n",
    "        # Encode the byte data to base64\n",
    "        img_base64 = base64.b64encode(img_byte)\n",
    "        # Convert base64 bytes to string\n",
    "        img_base64_string = img_base64.decode('utf-8')\n",
    "        return img_base64_string\n",
    "\n",
    "# Example usage\n",
    "image_path = IMG_PATH\n",
    "base64_string = image_to_base64(image_path)\n",
    "# print(base64_string)\n",
    "print(len(base64_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del VehicleImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='work-zone/Fase2/NEGATIVO/Tests/TEST FIAT - VIN CALIDAD ALTA.jpeg'> <class '_io.BufferedReader'>\n"
     ]
    },
    {
     "ename": "HttpResponseError",
     "evalue": "(InvalidRequest) The image size is not allowed to be zero or larger than 20971520 bytes.\nCode: InvalidRequest\nMessage: The image size is not allowed to be zero or larger than 20971520 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_info\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphoto_type\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVIN\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 29\u001b[0m             \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m                            \u001b[38;5;66;03m# [NEXT] clean text / URL\u001b[39;00m\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;66;03m# if img.content['photo_type']['label'] == 'DOCUMENTO':\u001b[39;00m\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;66;03m#     img.extract_entities_from_doc()                                 # [NEXT] use confidence\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# if (img.content['n_info'] >= 3):\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m#     img.content['status'] = 'REVIEW'\u001b[39;00m\n\u001b[0;32m     43\u001b[0m pprint(img\u001b[38;5;241m.\u001b[39mcontent, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, sort_dicts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\AndresFelipeHigueraR\\Downloads\\Images - INS\\vision_toolkit.py:244\u001b[0m, in \u001b[0;36mVehicleImage.extract_text\u001b[1;34m(self, content_type)\u001b[0m\n\u001b[0;32m    231\u001b[0m client \u001b[38;5;241m=\u001b[39m ImageAnalysisClient(\n\u001b[0;32m    232\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39mENDPOINT_PLATE_EXTRACTION,\n\u001b[0;32m    233\u001b[0m     credential\u001b[38;5;241m=\u001b[39mAzureKeyCredential(KEY_PLATE_EXTRACTION)\n\u001b[0;32m    234\u001b[0m )\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# if self.img_is_url:\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m#     pass\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# Extract text (OCR) from an image stream. This will be a \u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# synchronously (blocking) call.\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_binary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#image_data,\u001b[39;49;00m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisual_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mVisualFeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    250\u001b[0m lines \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mread[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblocks\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlines\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\AndresFelipeHigueraR\\Downloads\\Images - INS\\.venv\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\Users\\AndresFelipeHigueraR\\Downloads\\Images - INS\\.venv\\Lib\\site-packages\\azure\\ai\\vision\\imageanalysis\\_patch.py:140\u001b[0m, in \u001b[0;36mImageAnalysisClient.analyze\u001b[1;34m(self, image_data, visual_features, language, gender_neutral_caption, smart_crops_aspect_ratios, model_version, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs a single Image Analysis operation.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m:param image_data: A buffer containing the whole image to be analyzed.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m:raises: ~azure.core.exceptions.HttpResponseError\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    138\u001b[0m visual_features_impl: List[Union[\u001b[38;5;28mstr\u001b[39m, _models\u001b[38;5;241m.\u001b[39mVisualFeatures]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(visual_features)\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImageAnalysisClientOperationsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_analyze_from_image_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisual_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisual_features_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgender_neutral_caption\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgender_neutral_caption\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43msmart_crops_aspect_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmart_crops_aspect_ratios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AndresFelipeHigueraR\\Downloads\\Images - INS\\.venv\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\Users\\AndresFelipeHigueraR\\Downloads\\Images - INS\\.venv\\Lib\\site-packages\\azure\\ai\\vision\\imageanalysis\\_operations\\_operations.py:383\u001b[0m, in \u001b[0;36mImageAnalysisClientOperationsMixin._analyze_from_image_data\u001b[1;34m(self, image_content, visual_features, language, gender_neutral_caption, smart_crops_aspect_ratios, model_version, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m         response\u001b[38;5;241m.\u001b[39mread()  \u001b[38;5;66;03m# Load the body in memory and close the socket\u001b[39;00m\n\u001b[0;32m    382\u001b[0m     map_error(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m=\u001b[39mresponse, error_map\u001b[38;5;241m=\u001b[39merror_map)\n\u001b[1;32m--> 383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _stream:\n\u001b[0;32m    386\u001b[0m     deserialized \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39miter_bytes()\n",
      "\u001b[1;31mHttpResponseError\u001b[0m: (InvalidRequest) The image size is not allowed to be zero or larger than 20971520 bytes.\nCode: InvalidRequest\nMessage: The image size is not allowed to be zero or larger than 20971520 bytes."
     ]
    }
   ],
   "source": [
    "from vision_toolkit import VehicleImage\n",
    "from pprint import pprint\n",
    "\n",
    "IMG_PATH = 'work-zone/Fase2/NEGATIVO/Tests/TEST FIAT - VIN CALIDAD ALTA.jpeg' # VIN\n",
    "# IMG_PATH = 'work-zone/Fase2/DOCUMENTO/VXQ671_RFRTV.jpg'                 # DOCUMENTO: Plate/VIN\n",
    "# IMG_PATH = 'work-zone/Fase2/Automóvil/FRONTAL/1BLSR5_RFFRE.jpg'         # PLATE\n",
    "# IMG_PATH = 'work-zone/Fase2/Camión/INTERIOR/WZD418 - INTERIOR.jpeg'     # INTERIOR Camión\n",
    "# IMG_PATH = 'work-zone/Fase2/Motocicleta/FRONTAL/WLF34E - FRONTAL.jpeg'  # FRONTAL Motocicleta\n",
    "\n",
    "with open(IMG_PATH, 'rb') as img_binary:\n",
    "    input_request = {\n",
    "        'files': [img_binary]\n",
    "    }\n",
    "\n",
    "    # Load image\n",
    "    img = VehicleImage(img_binary)\n",
    "    img.show()\n",
    "\n",
    "    # Logic\n",
    "    img.classify_vehicle_photo()                                        # [NEXT] extract_text, option cropped\n",
    "\n",
    "    # No classification (likely due to image quality) -> REPEAT\n",
    "    if (img.content['n_info'] == 0):\n",
    "        img.content['status'] = 'REPEAT'\n",
    "\n",
    "    # Classification 1 (VIN, DOCUMENTO): Get VIN or Plate -> ???\n",
    "    if img.content['n_info'] == 1:\n",
    "        if img.content['photo_type']['label'] == 'VIN':\n",
    "            img.extract_text(content_type='vin')                            # [NEXT] clean text / URL\n",
    "        # if img.content['photo_type']['label'] == 'DOCUMENTO':\n",
    "        #     img.extract_entities_from_doc()                                 # [NEXT] use confidence\n",
    "\n",
    "    # # Classification 2 (FRONTAL R2, TRASERA R2): Get plata -> ???\n",
    "    # if img.content['n_info'] == 2:\n",
    "    #     if (img.content['photo_type']['label'] in ['FRONTAL', 'TRASERA']):  # [NEXT] MOTO\n",
    "    #         cropped_image = img.detect_plate()                              # [NEXT] don't save cropped img / URL\n",
    "    #     else:\n",
    "    #         img.content['status'] = 'SUCCESS'\n",
    "    # # Classification 3+: Model error -> REVIEW\n",
    "    # if (img.content['n_info'] >= 3):\n",
    "    #     img.content['status'] = 'REVIEW'\n",
    "\n",
    "pprint(img.content, width=200, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_image(img_binary):\n",
    "    img = plt.imread(img_binary)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "with open(IMG_PATH, 'rb') as f:\n",
    "    display_image(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = open(IMG_PATH, 'rb')\n",
    "test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup input\n",
    "# IMG_IS_URL = False\n",
    "\n",
    "# results = deepcopy(TEMPLATE)\n",
    "# results['img_location'] = IMG_PATH\n",
    "\n",
    "# # Classification \n",
    "# results = classify_image(                                               # DONE \n",
    "#     results=results,\n",
    "#     img_is_url=IMG_IS_URL\n",
    "# )\n",
    "\n",
    "# if (results['n_info'] == 0):                                            # DONE\n",
    "#     results['status'] = 'REPEAT'\n",
    "#     print('REPEAT')\n",
    "\n",
    "# if results['n_info'] == 1:\n",
    "#     if results['photo_type']['label'] == 'VIN':\n",
    "#         results = extract_text(\n",
    "#             content_type='vin',\n",
    "#             results=results, \n",
    "#             img_is_url=IMG_IS_URL\n",
    "#         )\n",
    "#         print('VIN')\n",
    "\n",
    "#     if results['photo_type']['label'] == 'DOCUMENTO':                   # TEST\n",
    "#         results = extract_from_doc(\n",
    "#             results=results,\n",
    "#             img_is_url=IMG_IS_URL\n",
    "#         )\n",
    "#         print('DOCUMENTO')\n",
    "\n",
    "# if results['n_info'] == 2:\n",
    "#     results, cropped_image = detect_plate(                              # NONE\n",
    "#         img_location=IMG_PATH, \n",
    "#         results=results,\n",
    "#         img_is_url=IMG_IS_URL\n",
    "#     )\n",
    "#     # Extract plate                                                     # NEXT\n",
    "#     print('OTHER')\n",
    "\n",
    "# if (results['n_info'] >= 3):\n",
    "#     results['status'] = 'REVIEW'\n",
    "#     print('REVIEW')\n",
    "\n",
    "# pprint(results, width=10, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "text = \"\"\"\n",
    "ÁÀÁÂB'BBBB/BBBBBB3B\n",
    "\"\"\"\n",
    "# Match:\n",
    "# 9BD363B32PYZ78594\n",
    "# 9BD3'63B32P'YZ78594\n",
    "# 9B333676738748898\n",
    "# 9B33367673/874/8898\n",
    "# ÁÀÁÂBBBBBBBBBBB3B\n",
    "\n",
    "# Don't match:\n",
    "# 9BD363B32PYZ7859\n",
    "# LONGWORDDDDDDDDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unidecode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munidecode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unidecode\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'unidecode'"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hola      andres'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = 'hola      andres'\n",
    "xx.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Cc': '\\n'},\n",
       " {'Lu': 'L'},\n",
       " {'Lu': 'C'},\n",
       " {'Lu': 'O'},\n",
       " {'Lu': 'C'},\n",
       " {'Lu': 'E'},\n",
       " {'Lu': 'A'},\n",
       " {'Lu': 'D'},\n",
       " {'Lu': 'B'},\n",
       " {'Nd': '2'},\n",
       " {'Nd': '2'},\n",
       " {'Nd': '0'},\n",
       " {'Nd': '0'},\n",
       " {'Nd': '0'},\n",
       " {'Nd': '0'},\n",
       " {'Nd': '0'},\n",
       " {'Nd': '0'},\n",
       " {'Nd': '6'},\n",
       " {'Lu': 'L'},\n",
       " {'Lu': 'C'},\n",
       " {'Lu': 'O'},\n",
       " {'Lu': 'C'},\n",
       " {'Lu': 'E'},\n",
       " {'Lu': 'A'},\n",
       " {'Lu': 'D'},\n",
       " {'Nd': '0'},\n",
       " {'Nd': '2'},\n",
       " {'Nd': '2'},\n",
       " {'Nd': '0'},\n",
       " {'Nd': '0'},\n",
       " {'Nd': '0'},\n",
       " {'Nd': '0'},\n",
       " {'Nd': '0'},\n",
       " {'Nd': '9'},\n",
       " {'Nd': '6'},\n",
       " {'Cc': '\\n'},\n",
       " {'Sm': '¬'},\n",
       " {'Sm': '|'},\n",
       " {'So': '°'},\n",
       " {'Sm': '+'},\n",
       " {'Sm': '~'},\n",
       " {'Sk': '¨'},\n",
       " {'Sk': '`'},\n",
       " {'Lu': 'Á'},\n",
       " {'Lu': 'À'},\n",
       " {'Lu': 'Á'},\n",
       " {'Lu': 'Â'},\n",
       " {'Lu': 'B'},\n",
       " {'Lu': 'B'},\n",
       " {'Lu': 'B'},\n",
       " {'Lu': 'B'},\n",
       " {'Lu': 'B'},\n",
       " {'Lu': 'B'},\n",
       " {'Lu': 'B'},\n",
       " {'Lu': 'B'},\n",
       " {'Lu': 'B'},\n",
       " {'Lu': 'B'},\n",
       " {'Lu': 'B'},\n",
       " {'Nd': '3'},\n",
       " {'Lu': 'B'},\n",
       " {'Cc': '\\n'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "text = \"\"\"\n",
    "LCOCEADB220000006/LCOCEAD0220000096\n",
    "¬|°*+~¨`\"ÁÀÁÂB'BBBB/BBB\\\\BBB3B\n",
    "\"\"\"\n",
    "text_without_accents = [\n",
    "    # Po (Other_Punctuation): \"'/*\n",
    "    # Sk ()\n",
    "    {unicodedata.category(char): char} for char in text \n",
    "    if unicodedata.category(char) not in ['Po']\n",
    "]\n",
    "# set([list(xx.values())[0] for xx in text_without_accents])\n",
    "text_without_accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ÅngsÁÀtröm123 hola\\n\"\n",
    "text_without_accents = ''.join(\n",
    "    char for char in normalized_text \n",
    "    if unicodedata.category(char) != 'Mn'\n",
    ")\n",
    "text_without_accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\nLCOCEADB220000006/LCOCEAD0220000096\\n¬|°*+~¨`\"ÁÀÁÂB\\'BBBB/BBB\\\\BBB3B\\n'\n",
      "'LCOCEADB220000006LCOCEAD0220000096AAAABBBBBBBBBBB3B'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_text = unicodedata.normalize('NFD', text)\n",
    "    \n",
    "# Remove accents by filtering out combining diacritical marks\n",
    "text_without_accents = ''.join(\n",
    "    char for char in normalized_text \n",
    "    if unicodedata.category(char) != 'Mn'\n",
    ")\n",
    "\n",
    "# Remove special characters by keeping only alphanumeric characters\n",
    "cleaned_text = re.sub(r'[^A-Za-z0-9]', '', text_without_accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx = eval(results['vin']['text'])\n",
    "for x in xx:\n",
    "    print()\n",
    "    print(x['text'])\n",
    "    print('\\t', [word['text'] for word in x['words']])\n",
    "    print('\\t', [word['confidence'] for word in x['words']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "root_dir = 'work-zone\\\\Fase2'\n",
    "data = {'vehicle_type': [], 'photo_type': [], 'img_path': []}\n",
    "\n",
    "\n",
    "for root, dirs, imgs in os.walk(root_dir):\n",
    "\n",
    "    vehicle_type = os.path.basename(os.path.dirname(root))\n",
    "    photo_type = os.path.basename(root)\n",
    "    \n",
    "    if vehicle_type.istitle():\n",
    "        for img in imgs:\n",
    "            img_path = os.path.join(root, img)\n",
    "            \n",
    "            data['vehicle_type'].append(vehicle_type)\n",
    "            data['photo_type'].append(photo_type)\n",
    "            data['img_path'].append(img_path)\n",
    "    else:\n",
    "        print(vehicle_type)\n",
    "        \n",
    "\n",
    "# Create a dataframe from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the dataframe\n",
    "df.groupby(['vehicle_type', 'photo_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "img_info = df['img_path'].str.extract(r'\\\\(?P<id>[A-Z0-9]+)(?P<id_type>\\s-\\s|_)')\n",
    "img_info['id_type'] = img_info['id_type'].replace({'_': 'FASE1', ' - ': 'FASE2'})\n",
    "# img_info#.groupby('id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "cond_document = (df['photo_type'] == 'DOCUMENTO')\n",
    "df[cond_document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "cond_plate_r2 = (df['vehicle_type'] == 'Motocicleta') &\\\n",
    "                (df['photo_type'] == 'TRASERA') \n",
    "cond_plate_r4 = df['vehicle_type'].isin(['Automóvil', 'Camión']) &\\\n",
    "                df['photo_type'  ].isin(['FRONTAL', 'TRASERA'])\n",
    "cond_plate    = cond_plate_r2 | cond_plate_r4\n",
    "\n",
    "df[cond_plate].groupby(['vehicle_type', 'photo_type']).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
